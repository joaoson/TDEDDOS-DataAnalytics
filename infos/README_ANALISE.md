# üìä Projeto DDoS Data Analytics - Guia de An√°lise

## üéØ Objetivo do Projeto

Aplicar t√©cnicas de **Data Analytics** em um cen√°rio pr√°tico de **detec√ß√£o de ataques DDoS**, atrav√©s de an√°lises univariada, multivariada, visualiza√ß√£o de dados e constru√ß√£o de modelos preditivos.

---

## üìÅ Estrutura dos Datasets

### Dataset Balanceado
- **Arquivo**: `ddos_balanced/final_dataset.csv`
- **Tamanho**: 6.48 GB
- **Caracter√≠sticas**: Distribui√ß√£o equilibrada de exemplos
- **Melhor para**: Aprender padr√µes com vi√©s reduzido

### Dataset Desbalanceado
- **Arquivo**: `ddos_imbalanced/unbalaced_20_80_dataset.csv`
- **Tamanho**: 3.93 GB
- **Caracter√≠sticas**: 20% DDoS, 80% Benign (realista)
- **Melhor para**: Treinar modelos que funcionem em produ√ß√£o

---

## üìö Guias Dispon√≠veis

| Documento | Descri√ß√£o |
|-----------|-----------|
| **ANALISE_DATASETS.md** | üìä An√°lise completa dos datasets com estat√≠sticas, estrutura e recomenda√ß√µes |
| **GUIA_PRATICO.md** | üõ†Ô∏è Snippets de c√≥digo prontos para executar em cada fase |
| Este arquivo | üìã Vis√£o geral e roteiro de execu√ß√£o |

---

## üöÄ Come√ßando R√°pido

### Pr√©-requisitos
```bash
pip install pandas numpy matplotlib seaborn scikit-learn xgboost
```

### Teste R√°pido dos Dados
```python
import pandas as pd

# Carregar amostra
df = pd.read_csv('ddos_balanced/final_dataset.csv', nrows=10000)
print(df.shape)
print(df['Label'].value_counts())
print(df.describe())
```

---

## üìä Fases da An√°lise

### **1. An√°lise Univariada**
Entender cada vari√°vel individualmente

**O que fazer:**
- [ ] Histogramas de frequ√™ncia
- [ ] Boxplots por classe (DDoS vs Benign)
- [ ] Estat√≠sticas descritivas
- [ ] Identificar outliers

**Vari√°veis-chave:**
- Taxa: `Flow Byts/s`, `Flow Pkts/s`
- Dura√ß√£o: `Flow Duration`
- Pacotes: `Tot Fwd Pkts`, `Tot Bwd Pkts`
- Flags TCP: `SYN Flag Cnt`, `ACK Flag Cnt`, `RST Flag Cnt`

**Ferramenta recomendada:** Pandas + Matplotlib/Seaborn

---

### **2. An√°lise Multivariada**
Entender relacionamentos entre vari√°veis

**O que fazer:**
- [ ] Matriz de correla√ß√£o
- [ ] Teste ANOVA para diferen√ßas entre classes
- [ ] An√°lise de Componentes Principais (PCA)
- [ ] Agrupamento explorat√≥rio (clustering)

**Esperado encontrar:**
- Padr√µes de comportamento DDoS distintos
- Vari√°veis mais discriminativas
- Redu√ß√£o de dimensionalidade via PCA

**Ferramenta recomendada:** Pandas + SciPy + Scikit-learn

---

### **3. Visualiza√ß√£o**
Comunicar padr√µes atrav√©s de gr√°ficos

**O que fazer:**
- [ ] Boxplots comparativos (DDoS vs Benign)
- [ ] Scatter plots de vari√°veis principais
- [ ] Heatmaps de correla√ß√£o
- [ ] Pair plots
- [ ] Distribui√ß√µes por classe

**Ferramenta recomendada:** Matplotlib + Seaborn

---

### **4. Classifica√ß√£o**
Construir modelo preditivo para detectar DDoS

**O que fazer:**
- [ ] Preparar dados (normalizar, feature selection)
- [ ] Treinar modelo baseline (Logistic Regression)
- [ ] Treinar modelos avan√ßados (Random Forest, XGBoost)
- [ ] Comparar desempenho
- [ ] Fine-tuning do melhor modelo
- [ ] An√°lise de features importantes

**Modelos recomendados:**
1. **Logistic Regression** (baseline r√°pido)
2. **Random Forest** (interpretabilidade)
3. **XGBoost** (melhor desempenho)
4. **Neural Networks** (complexo, melhor AUC)

**M√©tricas:**
- Accuracy (cuidado: desbalanceamento)
- Precision/Recall/F1-Score
- ROC-AUC (recomendado)
- Confusion Matrix

**Ferramenta recomendada:** Scikit-learn + XGBoost

---

## üìà Matriz de Colunas vs An√°lise

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Tipo de Coluna                      ‚îÇ Univariada‚îÇ Multivariada ‚îÇ Classifica√ß√£o
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Taxa/Velocidade (Flow Byts/s)       ‚îÇ    ‚úÖ    ‚îÇ      ‚úÖ      ‚îÇ     ‚úÖ     ‚îÇ
‚îÇ Dura√ß√£o (Flow Duration)             ‚îÇ    ‚úÖ    ‚îÇ      ‚úÖ      ‚îÇ     ‚úÖ     ‚îÇ
‚îÇ Pacotes (Tot Fwd/Bwd Pkts)          ‚îÇ    ‚úÖ    ‚îÇ      ‚úÖ      ‚îÇ     ‚úÖ     ‚îÇ
‚îÇ Comprimento pacotes (Pkt Len)       ‚îÇ    ‚úÖ    ‚îÇ      ‚úÖ      ‚îÇ     ‚úÖ     ‚îÇ
‚îÇ Flags TCP (SYN, ACK, RST)           ‚îÇ    ‚úÖ    ‚îÇ      ‚úÖ      ‚îÇ     ‚úÖ     ‚îÇ
‚îÇ Estat√≠sticas IAT (Inter-Arrival)    ‚îÇ    ‚úÖ    ‚îÇ      ‚úÖ      ‚îÇ     ‚úÖ     ‚îÇ
‚îÇ IDs e IPs (Flow ID, Src IP)         ‚îÇ    ‚ùå    ‚îÇ      ‚ö†Ô∏è      ‚îÇ     ‚ùå     ‚îÇ
‚îÇ Timestamp                           ‚îÇ    ‚ö†Ô∏è    ‚îÇ      ‚ö†Ô∏è      ‚îÇ     ‚ùå     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚úÖ = Usar diretamente
‚ö†Ô∏è  = Usar com cuidado (agrupar/agregar)
‚ùå = N√£o usar ou descartar
```

---

## üí° Insights Esperados

### Caracter√≠sticas de Ataque DDoS
- **Taxa elevada** de pacotes por segundo
- **Padr√£o repetitivo** de tipos de pacotes
- **Raz√£o anormal** entre Fwd e Bwd
- **Flags TCP anormais** (muitos SYN, poucos ACK)
- **Comprimento consistente** de pacotes
- **Dura√ß√£o longa** do fluxo

### Caracter√≠sticas de Tr√°fego Benign
- **Taxa variada** de pacotes
- **Padr√£o misto** de tipos
- **Raz√£o Fwd/Bwd balanceada**
- **Flags TCP normais**
- **Varia√ß√£o** no comprimento de pacotes
- **Dura√ß√£o curta** a m√©dia

---

## üîç Checklist de Execu√ß√£o

### Fase 1: Setup
- [ ] Verificar dataset est√° acess√≠vel
- [ ] Instalar bibliotecas necess√°rias
- [ ] Carregar amostra dos dados
- [ ] Entender estrutura b√°sica

### Fase 2: Explora√ß√£o
- [ ] Executar an√°lise univariada
- [ ] Criar histogramas e boxplots
- [ ] Documentar distribui√ß√µes
- [ ] Identificar colunas mais importantes

### Fase 3: An√°lise Multivariada
- [ ] Calcular correla√ß√µes
- [ ] Executar PCA
- [ ] Realizar testes estat√≠sticos
- [ ] Documentar padr√µes encontrados

### Fase 4: Visualiza√ß√µes
- [ ] Criar pair plots
- [ ] Fazer heatmaps
- [ ] Gr√°ficos comparativos por classe
- [ ] Preparar para apresenta√ß√£o

### Fase 5: Modelagem
- [ ] Preparar dados (normalizar, split)
- [ ] Treinar baseline
- [ ] Treinar modelos avan√ßados
- [ ] Comparar desempenho
- [ ] Fine-tuning

### Fase 6: Documenta√ß√£o
- [ ] Summarizar descobertas
- [ ] Escrever conclus√µes
- [ ] Documentar limita√ß√µes
- [ ] Propor trabalhos futuros

---

## üìä M√©tricas de Sucesso

| M√©trica | Bom | Excelente |
|---------|-----|-----------|
| **ROC-AUC** | > 0.85 | > 0.95 |
| **Precision** | > 0.80 | > 0.95 |
| **Recall** | > 0.80 | > 0.95 |
| **F1-Score** | > 0.80 | > 0.90 |

---

## ‚öôÔ∏è Dicas para Lidar com Dados Grandes

### Para ler arquivo grande em partes:
```python
# Op√ß√£o 1: nrows
df = pd.read_csv('arquivo.csv', nrows=100000)

# Op√ß√£o 2: skiprows
df = pd.read_csv('arquivo.csv', skiprows=range(1, 1000000), nrows=100000)

# Op√ß√£o 3: chunks
for chunk in pd.read_csv('arquivo.csv', chunksize=100000):
    process(chunk)
```

### Para salvar resultados:
```python
# Modelos
import joblib
joblib.dump(model, 'model.pkl')

# Dados
df.to_csv('resultados.csv', index=False)

# Metadados
import json
with open('metadata.json', 'w') as f:
    json.dump(metadata, f)
```

---

## üìö Refer√™ncias √öteis

- **Pandas**: https://pandas.pydata.org/
- **Matplotlib**: https://matplotlib.org/
- **Seaborn**: https://seaborn.pydata.org/
- **Scikit-learn**: https://scikit-learn.org/
- **XGBoost**: https://xgboost.readthedocs.io/

---

## ü§ù Pr√≥ximos Passos

1. **Leia ANALISE_DATASETS.md** para entender os dados em detalhe
2. **Consulte GUIA_PRATICO.md** para snippets de c√≥digo
3. **Comece com explora√ß√£o inicial** (rode o c√≥digo de teste r√°pido)
4. **Siga as fases** na ordem sugerida
5. **Documente suas descobertas** ao longo do caminho
6. **Crie um notebook Jupyter** compilando tudo

---

## ‚ùì D√∫vidas Comuns

**P: Por que dois datasets diferentes?**
R: Balanceado para aprender padr√µes, desbalanceado para simular produ√ß√£o.

**P: Por onde come√ßo?**
R: Comece com an√°lise univariada do dataset balanceado (menor), depois compare com desbalanceado.

**P: Devo usar todas as 85 colunas?**
R: N√£o. Feature selection reduz dimensionalidade e overfitting.

**P: Qual modelo √© melhor?**
R: Depende do trade-off: Logistic Regression √© r√°pida, Random Forest √© balanceada, XGBoost geralmente melhor AUC.

**P: Como lidar com desbalanceamento?**
R: Class weights, SMOTE, threshold tuning, stratified cross-validation.

---

**√öltima atualiza√ß√£o**: 2025-11-08
**Autor**: Claude Code
**Status**: üü¢ Pronto para come√ßar
